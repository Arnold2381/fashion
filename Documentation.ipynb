{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CODE_NAME_GRID.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5SB_T_wiuRB",
        "colab_type": "text"
      },
      "source": [
        "# Installing required libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiT0CVKkZ7uk",
        "colab_type": "text"
      },
      "source": [
        "Libraries used:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-31aQg9aC3Y",
        "colab_type": "text"
      },
      "source": [
        "1. simple_image_download - it returns the urls derived from the google image search of the fashion keywords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWeplPf0aTnx",
        "colab_type": "text"
      },
      "source": [
        "2. flask - it helps us to host our UI "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqZea7Hlage2",
        "colab_type": "text"
      },
      "source": [
        "3. textrazor - an NLP API for keyword extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S32h9wwGasiy",
        "colab_type": "text"
      },
      "source": [
        "4. flair - NLP Framework for POS tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVoZ_ZmAazFc",
        "colab_type": "text"
      },
      "source": [
        "5. pytrends - an API for analysing data from Google Trends"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg0orUw14IYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/RiddlerQ/simple_image_download.git@2eb34f88bd275723809193b5d9349866cca048aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcner0qOftNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install Flask-SocketIO\n",
        "!pip install textrazor\n",
        "!pip install pytrends\n",
        "!pip install flair\n",
        "!pip install flask-ngrok\n",
        "!pip install flask==0.12.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7W1Pv5FYBYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install Werkzeug==0.16.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CXM12x_i7O7",
        "colab_type": "text"
      },
      "source": [
        "# Importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ENN-pm_i7ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask import Flask, flash, redirect, render_template, request, session, abort , url_for,jsonify,Response,session,request,g,make_response\n",
        "from flask_socketio import SocketIO\n",
        "import time,glob,random,sqlite3,os,requests\n",
        "from flask_ngrok import run_with_ngrok\n",
        "import requests\n",
        "from bs4 import BeautifulSoup,SoupStrainer\n",
        "import httplib2\n",
        "from googlesearch import search \n",
        "import os\n",
        "import unicodedata\n",
        "import re\n",
        "import textrazor\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "import time\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from simple_image_download import simple_image_download as simp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwcWW-PYjFaA",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW2JK9InkTLc",
        "colab_type": "text"
      },
      "source": [
        "Mounting Google Drive to our session storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP8Lh8uqZGpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy01Zcd4klfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "!mkdir GRID\n",
        "%cd /content/drive/My\\ Drive/GRID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buerJhyWk9qP",
        "colab_type": "text"
      },
      "source": [
        "Declaring the global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3En1g5kkzyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url_best = []\n",
        "best_name=[]\n",
        "s=''\n",
        "g=''\n",
        "p=''\n",
        "url_t=[]\n",
        "url_l=[]\n",
        "url_n=[]\n",
        "bug=[]\n",
        "keyword_t=[]\n",
        "keyword_l=[]\n",
        "nodata=[]\n",
        "final_tags=[]\n",
        "final_entities=[]\n",
        "final_images_dir=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y94cVQA0lSKw",
        "colab_type": "text"
      },
      "source": [
        "## Code to run webpage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OU_VqW-lFvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "app.secret_key=os.urandom(24)\n",
        "socketio = SocketIO(app)\n",
        "\n",
        "#code for the landing page\n",
        "@app.route('/',methods=['POST','GET'])\n",
        "def login():\n",
        "\tglobal s\n",
        "\tglobal g\n",
        "\tglobal p\n",
        "\tif(request.form.get('type')=='Trend'):\n",
        "\t\treturn render_template('trending.html',site=s,garment=g,url_trend=url_t,keyword_trend=keyword_t,len=len(url_t))\n",
        "\telif(request.form.get('type')=='Lag'):\n",
        "\t\treturn render_template('lagging.html',site=s,garment=g,url_lag=url_l,keyword_lag=keyword_l,len=len(url_l),len_nodata=len(url_n),url_nodata=url_n,keyword_nodata=nodata)\n",
        "\telif(request.form.get('type')=='Best'):\n",
        "\t\treturn render_template('best_sellers.html',site=s,garment=g,url_best=url_best,best_seller=best_name,len=len(url_best))\n",
        "\telif(request.form.get('type')=='Upcoming'):\n",
        "\t\treturn render_template('portals.html',portal=p,garment=g,len=len(final_images_dir),images=final_images_dir,filters=final_tags,entities=final_entities,len1=bug)\n",
        "\tif(request.form.get('site')!=None):\n",
        "\t\ts=request.form.get('site')\n",
        "\t\tg=request.form.get('garment')\n",
        "\t\tp=request.form.get('portal')\n",
        "\t\tsearch(g,s)\n",
        "\t\treturn render_template('overview.html',site=s,portal=p,garment=g,url_trend=url_t,url_lag=url_l,best=url_best,images=final_images_dir)   \n",
        "\treturn render_template('fashion.html')\n",
        " \n",
        "#code for the t-shirt simulator\n",
        "@app.route(\"/simulator/\", methods=['POST'])\n",
        "def move_forward():\n",
        "\t\tforward_message = \"Moving Forward...\"\n",
        "\t\treturn render_template('simulator.html', forward_message=forward_message);\n",
        "\n",
        "#code for the chatbot\n",
        "@app.route('/getd/<data>')\n",
        "def getd(data):\n",
        "        result=''\t\n",
        "        l=[]\n",
        "        page=data\t\n",
        "        api_key = \"a9170a3ab84748cc681f9fe929b19808\"\n",
        "        name=str(page)\n",
        "        x=name.split()\n",
        "        aa=x[0]\n",
        "        age=int(x[1])\t\n",
        "        base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
        "        city_name=aa\n",
        "  \n",
        "# complete_url variable to store \n",
        "# complete url address \n",
        "        complete_url = base_url + \"appid=\" + api_key + \"&q=\" + city_name \n",
        "      \n",
        "    # get method of requests module \n",
        "    # return response object \n",
        "        response = requests.get(complete_url) \n",
        "        x = response.json() \n",
        "      \n",
        "    # Now x contains list of nested dictionaries \n",
        "    # Check the value of \"cod\" key is equal to \n",
        "    # \"404\", means city is found otherwise, \n",
        "    # city is not found \n",
        "        if x[\"cod\"] != \"404\": \n",
        "      \n",
        "        # store the value of \"main\" \n",
        "        # key in variable y \n",
        "             y = x[\"main\"] \n",
        "      \n",
        "        # store the value corresponding \n",
        "        # to the \"temp\" key of y \n",
        "             current_temperature = y[\"temp\"] \n",
        "      \n",
        "        # store the value corresponding \n",
        "        # to the \"pressure\" key of y \n",
        "             current_pressure = y[\"pressure\"] \n",
        "      \n",
        "        # store the value corresponding \n",
        "        # to the \"humidity\" key of y \n",
        "             current_humidiy = y[\"humidity\"] \n",
        "      \n",
        "        # store the value of \"weather\" \n",
        "        # key in variable z \n",
        "             z = x[\"weather\"] \n",
        "      \n",
        "        # store the value corresponding  \n",
        "        # to the \"description\" key at  \n",
        "        # the 0th index of z \n",
        "             weather_description = z[0][\"description\"] \n",
        "             l=[str(current_temperature-273),str(current_pressure),str(current_humidiy), str(weather_description)]\n",
        "        # print following values \n",
        "             current_temperature-=273\n",
        "        if(len(l)==0):\n",
        "                result=\"City not found\"\n",
        "\n",
        "        elif(current_temperature<10):\n",
        "                result=\"Farishta suggests you to design woollen sweaters or jackets of \"\n",
        "                if(age<=15):\n",
        "                        result+=\"bright colours.\"\n",
        "                else:\n",
        "                        result+=\"dark colours.\"\t\n",
        "        elif(current_temperature>=20 and age<=12):\n",
        "             result=\"Farishta suggests you to design short sleeve t-shirts of light colour such as white,yellow, etc and prints of cartoons characters like Chhota Bheem, Doraemon, Spiderman etc \"\n",
        "        elif(current_temperature>=20 and age>12 and age<=18):\n",
        "             result=\"Farishta suggests you to design cotton short sleeve t-shirts of light colour such as white,yellow, etc and prints of sports, sportsman, education, science etc.\"\n",
        "        elif(current_temperature>=20and age>18 and age<=30):\n",
        "             result=\"Farishta suggests you to design cotton short sleeve casual t-shirts.\"\n",
        "        elif(current_temperature>=20and age>30 and age<=50):\n",
        "             result=\"Farishta suggests you to design cotton short sleeve solid light colour t-shirts.\"\t\t\n",
        "\n",
        "        elif(current_temperature<20 and age<=12):\n",
        "             result=\"Farishta suggests you to design full sleeve t-shirts of dark colour such as black, brown, etc and cartoons characters like Chhota Bheem, Doraemon, Spiderman etc \"\n",
        "        elif(current_temperature<20 and age>12 and age<18):\n",
        "             result=\"Farishta suggests you to design short sleeve t-shirts of light colour such as white,yellow, etc and prints of sports, sportsman, education, science etc.\"\n",
        "        elif(current_temperature<20 and age>=18 and age<=30):\n",
        "             result=\"Farishta suggests you to design hoodies with fictional characters on it.\"\n",
        "        elif(current_temperature<20 and age>30 and age<=50):\n",
        "             result=\"Farishta suggests you to design full sleeve solid dark colour t-shirts with some quotes.\"\n",
        "        else:\n",
        "             result=\"Are you comedy me?\\n\"\n",
        "        result+=\"\\nFarishta suggests you please checkout our T-shirt simulator feature. Its still a beta product will make it final soon.\"\n",
        "        return jsonify(r=result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDqNwA5vlqj0",
        "colab_type": "text"
      },
      "source": [
        "# Getting images from query sites\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2DXJe4Tl9R2",
        "colab_type": "text"
      },
      "source": [
        " Once the trending and lagging attributes are understood, we make a google search of those attributes to obtain images to serve the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8a3z2m7lgNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search(query,brand):\n",
        "    c=0\n",
        "    fashion=''\n",
        "    global trending\n",
        "    global lagging\n",
        "    trending={}\n",
        "    lagging={}\n",
        "    global url_t\n",
        "    global url_l\n",
        "    global url_n\n",
        "    global nodata\n",
        "    url_t=[]\n",
        "    url_l=[]\n",
        "    url_n=[]\n",
        "    nodata=[]\n",
        "    global final_tags\n",
        "    global final_entities\n",
        "    global final_images_dir\n",
        "    final_tags=[]\n",
        "    final_entities=[]\n",
        "    final_images_dir=[]\n",
        "    try: \n",
        "    \tfrom googlesearch import search \n",
        "    except ImportError: \n",
        "    \tprint(\"No module named 'google' found\") \n",
        "    \n",
        "    #the query is processed according to the site chosen\n",
        "    for j in search(query+\" \"+brand, tld=\"co.in\", num=10, stop=10, pause=2): \n",
        "        if(query.casefold() in j or brand.casefold() in j):\n",
        "          if(s.casefold()=='koovs'):\n",
        "            koovs(j)\n",
        "          elif(s.casefold()=='paytm-mall'):\n",
        "            paytm(j)\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-qpWxS7nKnE",
        "colab_type": "text"
      },
      "source": [
        "# Getting trending and lagging attributes from e-commerce sites."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOCIKE0SoH0P",
        "colab_type": "text"
      },
      "source": [
        "We utilise the DOM structure of the respective website to obtain information required to predict trending and lagging features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vMdyBVbnXBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def koovs(a):\n",
        "    global c\n",
        "    http = httplib2.Http()\n",
        "    status, response = http.request(a)\n",
        "    soup=BeautifulSoup(response,'lxml' )\n",
        "    colors=\"\"\n",
        "    colo=[]\n",
        "    brand=[]\n",
        "    material=[]\n",
        "    others=[]\n",
        "    spare=[]\n",
        "    koovs_trend=[]\n",
        "    desc=[]\n",
        "    for lnk in BeautifulSoup(response, 'html.parser',parse_only=SoupStrainer('img')):\n",
        "      if(lnk.has_attr('src') and 'product' in lnk['src'] and 'jpg' in lnk['src'] and lnk.has_attr('alt')):    \n",
        "        koovs_trend.append(lnk['src'])\n",
        "        desc.append(lnk['alt'])\n",
        "    #due to the order in which images load during scraping, we pick the appropriate ones\n",
        "    koovs_trend=koovs_trend[2:12]\n",
        "    desc=desc[2:12]\n",
        "    global url_best\n",
        "    global best_name\n",
        "    url_best=koovs_trend\n",
        "    best_name=desc\n",
        "\n",
        "    for link in BeautifulSoup(response, 'html.parser',parse_only=SoupStrainer('a')):\n",
        "        if (link.has_attr('href') and 'html' in link['href'] ) :\n",
        "            s=\"https://www.koovs.com\"+link['href']\n",
        "            status1, response1 = http.request(s)\n",
        "            for spa in BeautifulSoup(response1,'html.parser',parse_only=SoupStrainer(class_='breadcrumb')):\n",
        "                colors+=(spa.text+\" \")\n",
        "            for sp in BeautifulSoup(response1,'html.parser',parse_only=SoupStrainer(class_='desc')):\n",
        "                clean_text = unicodedata.normalize(\"NFKD\",sp.text)\n",
        "                spare.append(clean_text.split('\\n'))\n",
        "               \n",
        "    #we utilise specifics of site to extract info from the site\n",
        "    for i in spare:\n",
        "        if(' ' in i):\n",
        "            i.remove(' ')\n",
        "        i.remove('')\n",
        "        brand.append(i[0][(i[0].index('by')+2):].strip())\n",
        "        line = i[1]\n",
        "        matchObj = re.match( r'Made from (.*)', line, re.M|re.I)\n",
        "\n",
        "        if matchObj:\n",
        "          material.append( matchObj.group(1).strip())\n",
        "\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        for j in range(3,len(i)):\n",
        "            if(i[j]!=''):\n",
        "                others.append(i[j].strip())\n",
        "        \n",
        "\n",
        "    colors=colors.split()\n",
        "    colors1=['alice blue', 'antique white', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque', 'black', 'blanched almond', 'blue', 'blue violet', 'brown', 'burly wood', 'cadet blue', 'chartreuse', 'chocolate', 'coral', 'cornflower blue', 'cornsilk', 'crimson', 'cyan', 'dark blue', 'dark cyan', 'dark golden rod', 'dark gray', 'dark grey', 'dark green', 'dark khaki', 'dark magenta', 'dark olive green', 'dark orange', 'dark orchid', 'dark red', 'dark salmon', 'dark sea green', 'dark slate blue', 'dark slate gray', 'dark slate grey', 'dark turquoise', 'dark violet', 'deep pink', 'deep sky blue', 'dim gray', 'dim grey', 'dodger blue', 'fire brick', 'floral white', 'forest green', 'fuchsia', 'gainsboro', 'ghost white', 'gold', 'golden rod', 'gray', 'grey', 'green', 'green yellow', 'honey dew', 'hot pink', 'indian red', 'indigo', 'ivory', 'khaki', 'lavender', 'lavender blush', 'lawn green', 'lemon chiffon', 'light blue', 'light coral', 'light cyan', 'light golden rod yellow', 'light gray', 'light grey', 'light green', 'light pink', 'light salmon', 'light sea green', 'light sky blue', 'light slate gray', 'light slate grey', 'light steel blue', 'light yellow', 'lime', 'lime green', 'linen', 'magenta', 'maroon', 'medium aqua marine', 'medium blue', 'medium orchid', 'medium purple', 'medium sea green', 'medium slate blue', 'medium spring green', 'medium turquoise', 'medium violet red', 'midnight blue', 'mint cream', 'misty rose', 'moccasin', 'navajo white', 'navy', 'old lace', 'olive', 'olive drab', 'orange', 'orange red', 'orchid', 'pale golden rod', 'pale green', 'pale turquoise', 'pale violet red', 'papaya whip', 'peach puff', 'peru', 'pink', 'plum', 'powder blue', 'purple', 'rebecca purple', 'red', 'rosy brown', 'royal blue', 'saddle brown', 'salmon', 'sandy brown', 'sea green', 'sea shell', 'sienna', 'silver', 'sky blue', 'slate blue', 'slate gray', 'slate grey', 'snow', 'spring green', 'steel blue', 'tan', 'teal', 'thistle', 'tomato', 'turquoise', 'violet', 'wheat', 'white', 'white smoke', 'yellow', 'yellow green']\n",
        "    for i in (colors):\n",
        "        \n",
        "        if(i.casefold() in colors1):\n",
        "            colo.append(i)\n",
        "\n",
        "    coloring=set(colo) \n",
        "     \n",
        "    uw=set(others)\n",
        "    uw1=set(brand)\n",
        "    uw2=set(material)\n",
        "    brand_count=[]\n",
        "\n",
        "    duw1={}\n",
        "    duw2={}\n",
        "    duw3={}\n",
        "    duw4={}\n",
        "    for i in uw1:\n",
        "        duw1[i]=brand.count(i)\n",
        "    for i in uw:\n",
        "        duw2[i]=others.count(i)\n",
        "    for i in uw2:\n",
        "        duw3[i]=material.count(i)\n",
        "    for i in coloring:\n",
        "        duw4[i]=colo.count(i)\n",
        "    process(duw1,duw2,duw3,duw4)\n",
        "\n",
        "\n",
        "#similarly, the specifics of this site are used to extract our required information\n",
        "def paytm(a):\n",
        "    \n",
        "    global c\n",
        "    http = httplib2.Http()\n",
        "    status, response = http.request(a)\n",
        "    soup=BeautifulSoup(response,'lxml' )\n",
        "    colors=\"\"\n",
        "    types=['Product Type','Brand','Product Code','Color','Size','Material','Occasion','Length','Pattern','Sleeve','Neck Type','Fit','Gender','Hood','Set Contents','Wash Care','Disclaimer','Pattern Coverage','Combo Size']\n",
        "    colo=[]\n",
        "    brand=[]\n",
        "    material=[]\n",
        "    others=[]\n",
        "    spare=[]\n",
        "    koovs_trend=[]\n",
        "    desc=[]\n",
        "    coloring=[]\n",
        "    size=['S','M','L','XL']\n",
        "    for lnk in BeautifulSoup(response, 'html.parser',parse_only=SoupStrainer('img')):\n",
        "        if(lnk.has_attr('src') and 'product' in lnk['src'] and 'jpg' in lnk['src'] and lnk.has_attr('alt')):\n",
        "            \n",
        "            koovs_trend.append(lnk['src'])\n",
        "            desc.append(lnk['alt'])\n",
        "    koovs_trend=koovs_trend[2:12]\n",
        "    desc=desc[2:12]\n",
        "    global url_best\n",
        "    global best_name\n",
        "    url_best=koovs_trend\n",
        "    best_name=desc\n",
        "\n",
        "    for link in BeautifulSoup(response, 'html.parser',parse_only=SoupStrainer(class_='_8vVO')):\n",
        "   \n",
        "        if (link.has_attr('href')   and 'http' not in link['href'] ) :\n",
        "        \n",
        "            s='https://paytmmall.com'+link['href']\n",
        "            \n",
        "            status1, response1 = http.request(s)\n",
        "            others={}\n",
        "            for (sp,typ) in zip(BeautifulSoup(response1,'html.parser',parse_only=SoupStrainer(class_='_2LOI')),types):\n",
        "                others[typ]=sp.text\n",
        "            colo.append(others['Color'])\n",
        "            brand.append(others['Brand'])\n",
        "            if(others['Material'] not in size ):\n",
        "                material.append(others['Material'])\n",
        "            spare.append(others['Occasion'])\n",
        "            spare.append(others['Length'])\n",
        "            spare.append(others['Pattern'])\n",
        "            spare.append(others['Sleeve'])\n",
        "            spare.append(others['Neck Type'])\n",
        "            spare.append(others['Fit'])\n",
        "               \n",
        "    colors1=['alice blue', 'antique white', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque', 'black', 'blanched almond', 'blue', 'blue violet', 'brown', 'burly wood', 'cadet blue', 'chartreuse', 'chocolate', 'coral', 'cornflower blue', 'cornsilk', 'crimson', 'cyan', 'dark blue', 'dark cyan', 'dark golden rod', 'dark gray', 'dark grey', 'dark green', 'dark khaki', 'dark magenta', 'dark olive green', 'dark orange', 'dark orchid', 'dark red', 'dark salmon', 'dark sea green', 'dark slate blue', 'dark slate gray', 'dark slate grey', 'dark turquoise', 'dark violet', 'deep pink', 'deep sky blue', 'dim gray', 'dim grey', 'dodger blue', 'fire brick', 'floral white', 'forest green', 'fuchsia', 'gainsboro', 'ghost white', 'gold', 'golden rod', 'gray', 'grey', 'green', 'green yellow', 'honey dew', 'hot pink', 'indian red', 'indigo', 'ivory', 'khaki', 'lavender', 'lavender blush', 'lawn green', 'lemon chiffon', 'light blue', 'light coral', 'light cyan', 'light golden rod yellow', 'light gray', 'light grey', 'light green', 'light pink', 'light salmon', 'light sea green', 'light sky blue', 'light slate gray', 'light slate grey', 'light steel blue', 'light yellow', 'lime', 'lime green', 'linen', 'magenta', 'maroon', 'medium aqua marine', 'medium blue', 'medium orchid', 'medium purple', 'medium sea green', 'medium slate blue', 'medium spring green', 'medium turquoise', 'medium violet red', 'midnight blue', 'mint cream', 'misty rose', 'moccasin', 'navajo white', 'navy', 'old lace', 'olive', 'olive drab', 'orange', 'orange red', 'orchid', 'pale golden rod', 'pale green', 'pale turquoise', 'pale violet red', 'papaya whip', 'peach puff', 'peru', 'pink', 'plum', 'powder blue', 'purple', 'rebecca purple', 'red', 'rosy brown', 'royal blue', 'saddle brown', 'salmon', 'sandy brown', 'sea green', 'sea shell', 'sienna', 'silver', 'sky blue', 'slate blue', 'slate gray', 'slate grey', 'snow', 'spring green', 'steel blue', 'tan', 'teal', 'thistle', 'tomato', 'turquoise', 'violet', 'wheat', 'white', 'white smoke', 'yellow', 'yellow green']\n",
        "    for i in (colo):\n",
        "        \n",
        "        if(i.casefold() in colors1):\n",
        "            coloring.append(i)\n",
        "    colors=set(coloring)       \n",
        "    uw1=set(brand)\n",
        "    uw2=set(material)\n",
        "    \n",
        "    uw4=set(spare)\n",
        "    brand_count=[]\n",
        "    duw1={}\n",
        "    duw2={}\n",
        "    duw3={}\n",
        "    duw4={}\n",
        "    for i in uw1:\n",
        "        duw1[i]=brand.count(i)\n",
        "    for i in uw2:\n",
        "        duw2[i]=material.count(i)\n",
        "    for i in colors:\n",
        "        duw3[i]=coloring.count(i)\n",
        "    for i in uw4:\n",
        "        duw4[i]=spare.count(i)\n",
        "    process(duw1,duw4,duw2,duw3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKyL9ICfoo5q",
        "colab_type": "text"
      },
      "source": [
        "# Extracting the required trending and lagging features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC_Ls-tGo0UA",
        "colab_type": "text"
      },
      "source": [
        "We use keyword extraction methods using filters to extract fashion keywords from the information that was scraped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUV_6s-pppEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def process(duw1,duw2,duw3,duw4):\n",
        "    duw1={k: v for k, v in sorted(duw1.items(), key=lambda item: item[1],reverse=True)}\n",
        "    duw2={k: v for k, v in sorted(duw2.items(), key=lambda item: item[1],reverse=True)[:10]}\n",
        "    duw3={k: v for k, v in sorted(duw3.items(), key=lambda item: item[1],reverse=True)}\n",
        "    duw4={k: v for k, v in sorted(duw4.items(), key=lambda item: item[1],reverse=True)}\n",
        "    #keyword extraction methods to get fashion attributes\n",
        "    for i in duw2.keys():\n",
        "      client = textrazor.TextRazor(api_key=\"ef90a8665ca910ec35ee078d2414232e08eb3b393d160d05f7b738bf\", extractors=[\"words\",\"phrases\"])\n",
        "      client.set_entity_freebase_type_filters([\"/fashion/fashion_category\", \"/fashion/fashion_designer\",\"/fashion/fashion_label\",\"/fashion/fashion_week\",\"/fashion/fiber\",\"/fashion/garment\",\"/fashion/textile\",\"/fashion/weave\"])\n",
        "      to_analyze = i\n",
        "      response = client.analyze(to_analyze)\n",
        "      keywords = []\n",
        "      for np in response.noun_phrases():\n",
        "        keyword = to_analyze[np.words[0].input_start_offset: np.words[-1].input_end_offset]\n",
        "        keywords.append(keyword)\n",
        "      if len(keywords) >1:\n",
        "        keywords1 = keywords\n",
        "        for j in keywords1:\n",
        "          sentence = Sentence(j)\n",
        "          #utilisation of POS tagging\n",
        "          tagger = SequenceTagger.load('pos')\n",
        "          tagger.predict(sentence)\n",
        "          for entity in sentence.get_spans('pos'):\n",
        "            if (str(entity).split()[5]) == 'DT':\n",
        "              keywords.remove(j)\n",
        "      if(len(keywords)!=0):\n",
        "        csv_read(keywords[0])\n",
        "      else:\n",
        "        csv_read(i)\n",
        "    trend={k: v for k, v in sorted(trending.items(), key=lambda item: item[1])}\n",
        "    lag={k: v for k, v in sorted(lagging.items(), key=lambda item: item[1])}\n",
        "    garment=g\n",
        "    trend_search=[]\n",
        "    lag_search=[]\n",
        "    a=b=c=0\n",
        "    for i in trend:\n",
        "      keyword_t.append([list(duw4.keys())[a],list(duw1.keys())[b],list(duw3.keys())[c],i])\n",
        "      trend_search.append(list(duw4.keys())[a]+\" \"+list(duw1.keys())[b].casefold()+\" \"+list(duw3.keys())[c]+\" \"+i+\" \"+garment)\n",
        "      a=a+1\n",
        "      if(a==len(duw4)): a=a-1\n",
        "      b=b+1\n",
        "      if(b==len(duw1)): b=b-1\n",
        "      c=c+1\n",
        "      if(c==len(duw3)): c=c-1\n",
        "    print(trend_search)\n",
        "    a=len(duw4)-1\n",
        "    b=len(duw1)-1\n",
        "    c=len(duw3)-1\n",
        "    for i in lag:\n",
        "      keyword_l.append([list(duw4.keys())[a],list(duw1.keys())[b],list(duw3.keys())[c],i])\n",
        "      lag_search.append(list(duw4.keys())[a]+\" \"+list(duw1.keys())[b].casefold()+\" \"+list(duw3.keys())[c]+\" \"+i+\" \"+garment)\n",
        "      a=a-1\n",
        "      if(a==-1): a=0\n",
        "      b=b-1\n",
        "      if(b==-1): b=0\n",
        "      c=c-1\n",
        "      if(c==-1): c=0\n",
        "    print(lag_search)\n",
        "    print(nodata)\n",
        "    if(len(trend_search)!=0):\n",
        "      download(trend_search,6,1)\n",
        "    if(len(lag_search)!=0):\n",
        "      download(lag_search,6,2)\n",
        "    if(len(nodata)!=0):\n",
        "      download(nodata,2,3)\n",
        "    mega(g, p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0eVxnRqT0f",
        "colab_type": "text"
      },
      "source": [
        "The statistical search-related data of the keywords are stored as a csv file using Google Trends."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsaULnwyp8Dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def csv_read(string):\n",
        "  df2 = ['']\n",
        "  flag = True\n",
        "  startTime = time.time()\n",
        "  pytrend = TrendReq(hl='en-GB', tz=360)\n",
        "  df2[0]=string\n",
        "  dataset = []\n",
        "  for x in range(0,len(df2)):\n",
        "      keywords = [df2[x]]\n",
        "      pytrend.build_payload(\n",
        "      kw_list=keywords,\n",
        "      cat=68,\n",
        "      timeframe='2014-12-31 2020-03-01')\n",
        "      #we set a timeframe of past 5 years approx to get an idea of seasonal trends for each product/keyword\n",
        "      #different products/keywords have different periods in which they trend/lag and we extract these patterns\n",
        "      data = pytrend.interest_over_time()\n",
        "      if not data.empty:\n",
        "            data = data.drop(labels=['isPartial'],axis='columns')\n",
        "            dataset.append(data)\n",
        "  try:\n",
        "    result = pd.concat(dataset, axis=1)\n",
        "  except:\n",
        "    global nodata\n",
        "    nodata.append(string)\n",
        "    flag = False \n",
        "  if (flag == True):\n",
        "    result.to_csv('/content/GRID/'+'trends.csv')\n",
        "    predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDrYCgSfqZ_M",
        "colab_type": "text"
      },
      "source": [
        "We utilise the power of Google Trends which is freely available to predict which features are trending and lagging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF9FV9CDrKK7",
        "colab_type": "text"
      },
      "source": [
        "To do this, we analyze trends for each of the keyword during past years to see if it currently trending or lagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2yOuw42rSkz",
        "colab_type": "text"
      },
      "source": [
        "Utilising past trends allows us to avoid unicorns, which are sudden spike trends which die out soon. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRjoHoT1p_Fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict():\n",
        "  maxima={}\n",
        "  #with the patterns extracted from previous function, we find the times during which the sale increases and the times during which sales fall\n",
        "  #accordingly, we suggest if its desirable or undesirable to sell the product at this time of the year\n",
        "  with open('/content/GRID/trends.csv', 'r') as file:\n",
        "      reader = csv.reader(file)\n",
        "      i=-1\n",
        "      year = 2015\n",
        "      max = ['',-1]\n",
        "      for row in reader:\n",
        "          i=i+1\n",
        "          if(i<1):\n",
        "            fashion=row[1]\n",
        "            continue\n",
        "          if(year==2020): break\n",
        "          if(row[0].startswith(str(year))):\n",
        "              if(int(row[1])>max[1]): \n",
        "                  max[1]=int(row[1])\n",
        "                  max[0]=row[0]\n",
        "          else:\n",
        "              year=year+1\n",
        "              maxima[max[0]]=max[1]\n",
        "              max[1] = -1\n",
        "              max[0] = ''\n",
        "              if(int(row[1])>max[1]): \n",
        "                  max[1]=int(row[1])\n",
        "                  max[0]=row[0]\n",
        "  with open('/content/GRID/trends.csv', 'r') as file:\n",
        "      reader = csv.reader(file)\n",
        "      min = ['',101]\n",
        "      i=-1\n",
        "      num=0;\n",
        "      minima = {}\n",
        "      flag = False\n",
        "      keys = list(maxima.keys())\n",
        "      for row in reader:\n",
        "          i=i+1\n",
        "          if(i<1): continue\n",
        "          if(row[0]==keys[num]):\n",
        "              num=num+1\n",
        "              if(flag):\n",
        "                  minima[min[0]]=min[1]\n",
        "                  min[1] = 101\n",
        "                  min[0] = ''\n",
        "              if(num==5): break\n",
        "              flag = True\n",
        "          elif(flag and row[0]!=keys[0]):\n",
        "              if(int(row[1])<min[1]):\n",
        "                  min[1]=int(row[1])\n",
        "                  min[0]=row[0]\n",
        "  max_month=0\n",
        "  min_month=0\n",
        "  for x in maxima:\n",
        "      max_month=max_month+int(list(x.split(\"-\"))[1])\n",
        "  max_month=max_month/5\n",
        "  for x in minima:\n",
        "      min_month=min_month+int(list(x.split(\"-\"))[1])\n",
        "  min_month=min_month/4\n",
        "  today = datetime.today()\n",
        "  datem = datetime(today.year, today.month, 1)\n",
        "  s=str(datem)[5:7]\n",
        "  if(s[0]=='0'):\n",
        "    month=int(s[1])\n",
        "  else:\n",
        "    month=int(s)\n",
        "  if(max_month>min_month):\n",
        "      if(month<=min_month): \n",
        "        lagging[fashion]=float('{:.2f}'.format(min_month-month))\n",
        "      elif(month>min_month and month<=max_month): \n",
        "        trending[fashion]=float('{:.2f}'.format(max_month-month))\n",
        "      else: \n",
        "        lagging[fashion]=float('{:.2f}'.format(min_month+(12-month)))\n",
        "  else:\n",
        "      if(month<=max_month): \n",
        "        trending[fashion]=float('{:.2f}'.format(max_month-month))\n",
        "      elif(month>max_month and month<=min_month): \n",
        "        lagging[fashion]=float('{:.2f}'.format(min_month-month))\n",
        "      else: \n",
        "        trending[fashion]=float('{:.2f}'.format(max_month+(12-month)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HADO43Qnrn-F",
        "colab_type": "text"
      },
      "source": [
        "# Downloading the images of trending and lagging feature products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF3yPUxNru-_",
        "colab_type": "text"
      },
      "source": [
        "Once we identify the trending and lagging features, we are no longer constrained by the site to obtain pictures. We then proceed to obtain pictures for the products from Google search. These will be shown to the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvxfbqB4rine",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download(pics,n,num):\n",
        "  for i in pics:\n",
        "    response = simp.simple_image_download\n",
        "    if(num==1):url_t.append(response().urls(i,n))\n",
        "    elif(num==2):url_l.append(response().urls(i,n))\n",
        "    else:url_n.append(response().urls(i,n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rywGUkE3vAiq",
        "colab_type": "text"
      },
      "source": [
        "# Fashion Portals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhFfYXT4vF4c",
        "colab_type": "text"
      },
      "source": [
        "We enter the respective fashion portal and using keyword extraction to filter out the articles that contain our garment of interest. Once that is done, we proceed to obtain insights from our articles of interest. Entites like models, colors of garments, style of fashion and garments, etc are obtained. This can give a lot of insights to the the designer, as in which models are trending right now, the trending color, key locations where fashion is booming, etc. The model can even detect references to tv-shows, fictional characters, famous figures, cartoons, etc. Since fashion is influenced by a lot of features, we leave almost no stone unturned as in identifying  key factors driving fashion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubFw1lAi6aT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "822fca8b-37d5-424e-92ff-7f86920918aa"
      },
      "source": [
        "def mega(garment, site):\n",
        "\n",
        "  c=0\n",
        "  global final_entities\n",
        "  global final_images_dir\n",
        "  global final_tags\n",
        "  corpus_list=[]\n",
        "\n",
        "#since scraping code is not one size fits all, we choose the scraping code depending on the site.\n",
        "\n",
        "  def vogue():\n",
        "    \n",
        "      a=\"https://www.vogue.co.uk/fashion/fashion-trends\"\n",
        "      global c\n",
        "      http = httplib2.Http()\n",
        "      status, response = http.request(a)\n",
        "      soup=BeautifulSoup(response,'lxml' )\n",
        "\n",
        "      for link in BeautifulSoup(response, 'html.parser',parse_only=SoupStrainer('a')):\n",
        "\n",
        "          k=\"\"\n",
        "          if (link.has_attr('href') and 'article' in link['href'] ) :\n",
        "              s=\"https://www.vogue.co.uk\"+link['href']\n",
        "              k=k+s+'\\n'\n",
        "              status1, response1 = http.request(s)\n",
        "              for spa in BeautifulSoup(response1,'html.parser',parse_only=SoupStrainer('p')):\n",
        "                  k=k+spa.text\n",
        "              corpus_list.append(k)\n",
        "\n",
        "  def elle():\n",
        "    a=\"https://www.elle.com/fashion/trend-reports/\"\n",
        "    global c\n",
        "    http = httplib2.Http()\n",
        "    status, response = http.request(a)\n",
        "    soup=BeautifulSoup(response,'lxml' )\n",
        "    for link in BeautifulSoup(response, 'html.parser',parse_only=SoupStrainer('a')):\n",
        "\n",
        "        k=\"\"\n",
        "        if (link.has_attr('href') and 'trend-reports' in link['href'] and 'http' not in link['href'] ) :\n",
        "            s=\"https://www.elle.com\"+link['href']\n",
        "            k=k+s+'\\n'\n",
        "            status1, response1 = http.request(s)\n",
        "            for spa in BeautifulSoup(response1,'html.parser',parse_only=SoupStrainer('p')):\n",
        "                k=k+spa.text\n",
        "            corpus_list.append(k)\n",
        "\n",
        "  def esquire():\n",
        "    \n",
        "      a=\"https://www.esquire.com/style/mens-fashion/\"\n",
        "      global c\n",
        "      http = httplib2.Http()\n",
        "      status, response = http.request(a)\n",
        "      soup=BeautifulSoup(response,'lxml' )\n",
        "\n",
        "      for link in BeautifulSoup(response, 'html.parser',parse_only=SoupStrainer('a')):\n",
        "\n",
        "          k=\"\"\n",
        "          if (link.has_attr('href') and 'style/mens-fashion' in link['href'] and 'http' not in link['href']) :\n",
        "              #print(link['href'])\n",
        "              s=\"https://www.esquire.com/\"+link['href']\n",
        "              k=k+s+'\\n'\n",
        "              status1, response1 = http.request(s)\n",
        "              for spa in BeautifulSoup(response1,'html.parser',parse_only=SoupStrainer('p')):\n",
        "                  k=k+spa.text\n",
        "              corpus_list.append(k)\n",
        "\n",
        "  if p.casefold() == 'vogue':\n",
        "    vogue()\n",
        "\n",
        "  elif p .casefold() == 'esquire':\n",
        "    esquire()\n",
        "\n",
        "#since magazine titles are often misleading, we use keyword extraction to understand which garment the article deals with\n",
        "  def detect_target_garment(content, target):\n",
        "    client = textrazor.TextRazor(api_key=\"13b22cd6d8562948feeddee54a992ef4edfb1b9d8c3df54a70f40810\", extractors=[\"entities\"])\n",
        "    client.set_entity_freebase_type_filters([\"/fashion/garment\", \"/business/product_category\"])\n",
        "    to_analyze = content\n",
        "    response = client.analyze(to_analyze)\n",
        "    garment = []\n",
        "    for ent in response.entities():\n",
        "      garment.append(ent.id)\n",
        "    if target in garment:\n",
        "      return True\n",
        "\n",
        "#we filter out and retain only the articles of our interest\n",
        "  def  flush(corpus_list, target_garment):\n",
        "    refined_list = []\n",
        "    for i, article in enumerate(corpus_list[:-1]):\n",
        "      url = article.partition('\\n')[0]\n",
        "      content = article\n",
        "      res = detect_target_garment(content,target_garment)\n",
        "      if res == True:\n",
        "        refined_list.append(content)    \n",
        "    return refined_list\n",
        "\n",
        "  target = garment\n",
        "  refined_list = flush(corpus_list, target)\n",
        "\n",
        "  refined_urls = []\n",
        "  for i in refined_list:\n",
        "    refined_urls.append(i.partition('\\n')[0])\n",
        "\n",
        "#code is tweaked to extract pics from respective site of our interest\n",
        "  def refined_url_pics(url):\n",
        "    images_list = []\n",
        "    a=url\n",
        "    global c\n",
        "    http = httplib2.Http()\n",
        "    status, response = http.request(a)\n",
        "    soup=BeautifulSoup(response,'lxml' )\n",
        "    for link in BeautifulSoup(response, 'html.parser',parse_only=SoupStrainer('img')):\n",
        "\n",
        "        if (link.has_attr('srcset') and 'vogue' in url  ) :\n",
        "            l=(link['srcset'].split(','))\n",
        "            images_list.append(l[len(l)-1][:-6])\n",
        "        if (link.has_attr('data-src') and link.has_attr('alt') and link['alt']!='' and 'hips' in link['data-src'] and 'esquire' in url) :\n",
        "            images_list.append(link['data-src'])\n",
        "            break     \n",
        "            \n",
        "    return images_list\n",
        "\n",
        "  for url in refined_urls:\n",
        "    final_images_dir.append(refined_url_pics(url))\n",
        "\n",
        "  print(final_images_dir)\n",
        "\n",
        "#we use a huge list of filters to identify useful insights from the articles of our interest\n",
        "#the model we use is powerful enough to understand complex context as well\n",
        "#entities like model names, locations, fashion styles are picked up to keep the user informed of the current meta\n",
        "  filter_list = ['/fashion/fashion_category', '/fashion/fashion_label', '/fashion/fashion_week', '/fashion/fashion_designer', '/fashion/fiber', '/fashion/textile', '/fashion/weave', '/business/industry', '/business/product_category', '/business/product_line', '/business/sponsor', '/celebrities/celebrity', '/celebrities/sexual_orientation', '/celebrities/supercouple', '/comic_books/comic_book_character', '/comic_books/comic_book_fictional_universe', '/comic_strips/comic_strip', '/comic_strips/comic_strip_character', '/computer/computer_designer', '/cricket/cricket_player', '/exhibitions/exhibit', '/exhibitions/exhibition', '/fictional_universe/fictional_character', '/fictional_universe/fictional_object', '/fictional_universe/fictional_universe', '/fictional_universe/person_in_fiction', '/fictional_universe/work_of_fiction', '/film/film_character', '/film/film_genre', '/film/person_or_entity_appearing_in_film', '/internet/blog', '/internet/social_network_user', '/internet/website', '/location/citytown', '/location/country', '/location/region', '/media_common/creative_work', '/media_common/dedicated_work', '/media_common/finished_work', '/media_common/netflix_title', '/music/artist', '/music/festival', '/music/music_video_character', '/music/musical_group', '/music/producer', '/music/track', '/music/soundtrack', '/music/single', '/music/music_video', '/music/genre', '/organization/australian_organization', '/organization/club', '/organization/club_interest', '/organization/endowed_organization', '/organization/membership_organization', '/organization/non_profit_designation', '/organization/non_profit_organization', '/organization/organization', '/organization/organization_committee', '/organization/organization_committee_title',    '/organization/organization_type', '/tv/tv_genre', '/tv/tv_personality']\n",
        "\n",
        "  def tag_extractor(article):\n",
        "    client = textrazor.TextRazor(api_key=\"13b22cd6d8562948feeddee54a992ef4edfb1b9d8c3df54a70f40810\", extractors=[\"entities\"])\n",
        "    client.set_entity_freebase_type_filters(filter_list)\n",
        "    to_analyze = article\n",
        "    response = client.analyze(to_analyze)\n",
        "    tags_dir = {}\n",
        "    for ent in response.entities():\n",
        "      intersection = list(set(ent.freebase_types) & set(filter_list))\n",
        "      for i in intersection:\n",
        "        k = i[1:].upper()\n",
        "        if k  in tags_dir.keys():\n",
        "          tags_dir[k].append(ent.id)\n",
        "        else:\n",
        "          tags_dir[k] = [ent.id]\n",
        "    tags =[]\n",
        "    entities = []\n",
        "    \n",
        "    for i in tags_dir.keys():\n",
        "      entities.append(tags_dir[i])\n",
        "      tags.append(i)\n",
        "    return tags, entities\n",
        "\n",
        "  for i in refined_list:\n",
        "    a, b = tag_extractor(i)\n",
        "\n",
        "    final_tags.append(a)\n",
        "    final_entities.append(b)\n",
        "  len1=[]\n",
        "  for i in final_tags:\n",
        "    len1.append(len(i))\n",
        "  global bug\n",
        "  bug=len1\n",
        "  for i,x in enumerate(final_entities):\n",
        "    for j,y in enumerate(x):\n",
        "        final_entities[i][j] = list(set(y))\n",
        "  print(final_tags)\n",
        "  print(final_entities)\n",
        "\n",
        "#driver code to trigger our website\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-09 12:48:34,396 loading file /root/.flair/models/en-pos-ontonotes-v0.5.pt\n",
            "2020-08-09 12:48:35,011 loading file /root/.flair/models/en-pos-ontonotes-v0.5.pt\n",
            "['Black koovs cotton Muscle fit T-shirt', 'Blue chelsea king polyester blend Regular fit T-shirt', 'Green adidas originals polyester Long sleeves T-shirt', 'Pink adamo london cotton blend Slim fit T-shirt', 'Grey fila poly-spandex blend Half placket T-shirt']\n",
            "['Yellow k denim polyester-spandex blend Short sleeves T-shirt']\n",
            "['Side-split hem', 'Half-placket button closure', 'Half placket button closure', 'Contrast shoulder tape']\n",
            "[[' https://media.vogue.co.uk/photos/5eecb6815f6672410c7cdb85/2:3/w_3840%2cc_limit/GettyImages-1037457514.jpg', ' https://media.vogue.co.uk/photos/5e981bf8eb104e0008b35453/3:2/w_3840%2cc_limit/GettyImages-1133271785.jpg', ' https://media.vogue.co.uk/photos/5eecb67c5f6672410c7cdb81/master/w_3840%2cc_limit/GettyImages-1175606107.jpg', ' https://media.vogue.co.uk/photos/5eecb67f8d5f1419f3619883/master/w_3840%2cc_limit/GettyImages-1173486519.jpg', ' https://media.vogue.co.uk/photos/5eecb6805f6672410c7cdb83/master/w_3840%2cc_limit/GettyImages-1204749692.jpg', ' https://media.vogue.co.uk/photos/5eecb6908d5f1419f361988b/master/w_3840%2cc_limit/GettyImages-1222289900.jpg', ' https://media.vogue.co.uk/photos/5eecb6815f6672410c7cdb85/master/w_3840%2cc_limit/GettyImages-1037457514.jpg', ' https://media.vogue.co.uk/photos/5eecb68e8d5f1419f3619889/master/w_3840%2cc_limit/GettyImages-1221692520.jpg', ' https://media.vogue.co.uk/photos/5eecb6895f6672410c7cdb87/master/w_3840%2cc_limit/GettyImages-1218439888.jpg', ' https://media.vogue.co.uk/photos/5eecb689a2fb5e04492485ac/master/w_3840%2cc_limit/GettyImages-1220562811.jpg', ' https://media.vogue.co.uk/photos/5eecb6858d5f1419f3619885/master/w_3840%2cc_limit/GettyImages-1219645519.jpg', ' https://media.vogue.co.uk/photos/5eecb68c5f6672410c7cdb89/master/w_3840%2cc_limit/GettyImages-1222830273.jpg', ' https://media.vogue.co.uk/photos/5eecb6868d5f1419f3619887/master/w_3840%2cc_limit/GettyImages-1212192072.jpg', ' https://media.vogue.co.uk/photos/5ee35e0ce400ca1697b177f9/1:1/w_3840%2cc_limit/Jennifer%20Anniston%20sunglasses.jpg', ' https://media.vogue.co.uk/photos/5eeb6dfaa2fb5e0449248456/1:1/w_3840%2cc_limit/GettyImages-1182336664.jpg']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [09/Aug/2020 12:49:03] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['LOCATION/COUNTRY', 'BUSINESS/PRODUCT_CATEGORY', 'BUSINESS/INDUSTRY', 'FICTIONAL_UNIVERSE/PERSON_IN_FICTION', 'MUSIC/ARTIST', 'FILM/PERSON_OR_ENTITY_APPEARING_IN_FILM', 'CELEBRITIES/CELEBRITY', 'TV/TV_PERSONALITY', 'ORGANIZATION/ORGANIZATION', 'TV/TV_GENRE', 'FASHION/FASHION_LABEL', 'FILM/FILM_GENRE', 'ORGANIZATION/ORGANIZATION_TYPE', 'MEDIA_COMMON/NETFLIX_TITLE', 'FASHION/TEXTILE']]\n",
            "[[['United Kingdom', 'United States'], ['Blazer', 'Jacket', 'Leather jacket', 'T-shirt'], ['Fashion', 'T-shirt'], ['James Dean', 'Brooke Shields'], ['Christy Turlington', 'James Dean', 'Brooke Shields'], ['Cindy Crawford', 'James Dean', 'Christy Turlington', 'Brooke Shields'], ['Cindy Crawford', 'Christy Turlington', 'Brooke Shields'], ['Cindy Crawford'], ['Cond Nast', 'Cindy Crawford', 'Fashion'], ['Fashion'], ['Fashion'], ['Fashion'], ['Fashion'], ['Rebel Without a Cause'], ['Denim']]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [09/Aug/2020 12:51:12] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:51:35] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:51:58] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:52:12] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:52:54] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:53:22] \"\u001b[37mGET /getd/Auckland%2030 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:53:45] \"\u001b[37mGET /getd/Mumbai%2020 HTTP/1.1\u001b[0m\" 200 -\n",
            "[2020-08-09 12:53:59,552] ERROR in app: Exception on /getd/Bye [GET]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1982, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1614, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1517, in handle_user_exception\n",
            "    reraise(exc_type, exc_value, tb)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/_compat.py\", line 33, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1612, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1598, in dispatch_request\n",
            "    return self.view_functions[rule.endpoint](**req.view_args)\n",
            "  File \"<ipython-input-9-16ac9d8a239c>\", line 44, in getd\n",
            "    age=int(x[1])\n",
            "IndexError: list index out of range\n",
            "127.0.0.1 - - [09/Aug/2020 12:53:59] \"\u001b[1m\u001b[35mGET /getd/Bye HTTP/1.1\u001b[0m\" 500 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:54:01] \"\u001b[37mPOST /simulator/ HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Black nike Classic Canvas Sneakers', 'White foonike-men-s-tshak29399de787644 Athleisure Synthetic Sneakers', 'Blue foonike-men-s-tshak29399de787644 Slip-On Round Toe Sneakers', 'Grey foonike-men-s-tshak29399de787644 Black Lace-Up Sneakers', 'Green foonike-men-s-tshak29399de787644 High Top Suede Leather Sneakers', 'Green foonike-men-s-tshak29399de787644 Blue Solid Sneakers', 'Green foonike-men-s-tshak29399de787644 Grey Rubber Sneakers']\n",
            "['Green foonike-men-s-tshak29399de787644 Running Shoes Mesh Sneakers', 'Grey nike Grey EVA Sneakers', 'Blue nike Blue 1 Pair Sneakers']\n",
            "[]\n",
            "[['https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/bestof2020-1589485703.jpg?crop=0.502xw:1.00xh;0.498xw,0&resize=640:*'], ['https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/index-2-1596820440.png?crop=0.497xw:0.993xh;0.252xw,0&resize=640:*'], [], ['https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/hikingshoes-1596638780.jpg?crop=0.502xw:1.00xh;0.250xw,0&resize=640:*'], ['https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/wedding-2-3-1596584003.jpg?crop=0.502xw:1.00xh;0.250xw,0&resize=640:*'], ['https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/hdm-vans-041-1596578366.jpg?crop=1.00xw:1.00xh;0,0&resize=480:*'], ['https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/sneakers-1596112056.jpg?crop=0.502xw:1.00xh;0.250xw,0&resize=640:*'], ['https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/summer-1596134642.jpg?crop=0.502xw:1.00xh;0.250xw,0&resize=640:*']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [09/Aug/2020 12:57:04] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['ORGANIZATION/ORGANIZATION_TYPE', 'BUSINESS/INDUSTRY', 'BUSINESS/PRODUCT_CATEGORY', 'BUSINESS/PRODUCT_LINE', 'LOCATION/COUNTRY', 'ORGANIZATION/CLUB_INTEREST', 'LOCATION/CITYTOWN', 'BUSINESS/SPONSOR', 'ORGANIZATION/ORGANIZATION', 'CELEBRITIES/CELEBRITY', 'MUSIC/PRODUCER', 'FILM/PERSON_OR_ENTITY_APPEARING_IN_FILM', 'FASHION/FASHION_DESIGNER', 'INTERNET/SOCIAL_NETWORK_USER', 'TV/TV_PERSONALITY', 'MUSIC/ARTIST'], ['BUSINESS/PRODUCT_CATEGORY', 'BUSINESS/PRODUCT_LINE', 'BUSINESS/INDUSTRY', 'LOCATION/COUNTRY', 'BUSINESS/SPONSOR', 'ORGANIZATION/ORGANIZATION', 'LOCATION/CITYTOWN', 'ORGANIZATION/ORGANIZATION_TYPE', 'MUSIC/PRODUCER', 'MUSIC/ARTIST'], ['BUSINESS/SPONSOR', 'ORGANIZATION/ORGANIZATION', 'BUSINESS/PRODUCT_CATEGORY', 'BUSINESS/PRODUCT_LINE', 'BUSINESS/INDUSTRY', 'MUSIC/GENRE', 'MUSIC/ARTIST', 'LOCATION/REGION', 'LOCATION/CITYTOWN', 'MUSIC/PRODUCER', 'FILM/PERSON_OR_ENTITY_APPEARING_IN_FILM', 'CELEBRITIES/CELEBRITY', 'INTERNET/SOCIAL_NETWORK_USER'], ['LOCATION/CITYTOWN', 'BUSINESS/INDUSTRY', 'ORGANIZATION/CLUB_INTEREST', 'ORGANIZATION/ORGANIZATION_TYPE', 'TV/TV_GENRE', 'FILM/FILM_GENRE', 'BUSINESS/PRODUCT_CATEGORY', 'BUSINESS/PRODUCT_LINE', 'ORGANIZATION/ORGANIZATION', 'FICTIONAL_UNIVERSE/FICTIONAL_CHARACTER', 'FILM/FILM_CHARACTER', 'LOCATION/COUNTRY', 'LOCATION/REGION', 'BUSINESS/SPONSOR'], ['BUSINESS/PRODUCT_CATEGORY', 'MUSIC/GENRE', 'FILM/FILM_GENRE', 'MUSIC/PRODUCER', 'MUSIC/ARTIST', 'FASHION/TEXTILE', 'BUSINESS/INDUSTRY', 'LOCATION/COUNTRY', 'BUSINESS/PRODUCT_LINE', 'FICTIONAL_UNIVERSE/PERSON_IN_FICTION', 'FICTIONAL_UNIVERSE/FICTIONAL_CHARACTER', 'ORGANIZATION/ORGANIZATION_TYPE', 'FASHION/FIBER', 'ORGANIZATION/ORGANIZATION'], ['BUSINESS/INDUSTRY', 'BUSINESS/PRODUCT_CATEGORY', 'BUSINESS/PRODUCT_LINE', 'LOCATION/REGION', 'MUSIC/GENRE', 'ORGANIZATION/ORGANIZATION_TYPE', 'TV/TV_GENRE', 'FASHION/FASHION_LABEL', 'FILM/FILM_GENRE', 'ORGANIZATION/ORGANIZATION'], ['BUSINESS/PRODUCT_CATEGORY', 'BUSINESS/PRODUCT_LINE', 'BUSINESS/INDUSTRY', 'LOCATION/CITYTOWN', 'FICTIONAL_UNIVERSE/PERSON_IN_FICTION', 'LOCATION/COUNTRY', 'FICTIONAL_UNIVERSE/FICTIONAL_CHARACTER', 'ORGANIZATION/ORGANIZATION'], ['BUSINESS/PRODUCT_CATEGORY', 'FICTIONAL_UNIVERSE/PERSON_IN_FICTION', 'FASHION/FIBER', 'BUSINESS/PRODUCT_LINE', 'BUSINESS/INDUSTRY', 'ORGANIZATION/ORGANIZATION_TYPE', 'ORGANIZATION/CLUB_INTEREST', 'ORGANIZATION/ORGANIZATION', 'LOCATION/COUNTRY', 'LOCATION/REGION']]\n",
            "[[['Retail'], ['Shoe', 'Retail', 'Technology', 'Brand'], ['Shoe', 'Smartphone', 'Sneakers'], ['Shoe', 'Nike Air Max', 'Sneakers'], ['Jordan'], ['Association football'], ['Casablanca'], ['Nike, Inc.', 'Converse (shoe company)', 'New Balance'], ['Esquire', 'Air Max Day', 'NIKEThe Air Max', 'Atmos', 'NOWAdidas', 'Nike, Inc.', 'Red Cement', 'New Balance', 'AIR JORDAN Diehard Jordan', 'AIR JORDANHard', 'Converse (shoe company)'], ['Pharrell Williams'], ['Pharrell Williams'], ['Pharrell Williams'], ['Pharrell Williams'], ['Pharrell Williams'], ['Pharrell Williams'], ['Pharrell Williams']], [['Clog', 'Shoe', 'Clothing', 'Sneakers'], ['Shoe', 'Sneakers'], ['Shoe', 'Brand', 'Internet', 'Clothing'], ['Jordan', 'Australia', 'Canada'], ['Stssy', 'Nike, Inc.', 'Adidas', 'New Balance'], ['Stssy', 'A Bathing Ape', 'denimhead', 'Birkenstock', 'Esquire', 'footlocker.com', 'Nike, Inc.', 'Sky Force', 'Human Made', 'New Balance', 'Adidas'], ['Boston', 'Paris'], ['Clothing'], ['Nigo'], ['Nigo']], [['Nike, Inc.'], ['Esquire', 'Nike, Inc.', 'Roc-A-Fella Records', 'Air Force'], ['Shoe', 'Sneakers'], ['Shoe', 'Sneakers'], ['Shoe', 'Hip hop music'], ['Hip hop music'], ['G Herbo', 'Nelly', 'Damon Dash'], ['East Coast of the United States'], ['New York City'], ['Damon Dash'], ['Nelly', 'Damon Dash'], ['Nelly'], ['Nelly']], [['Soho'], ['Brand', 'Gear', 'Shoe', 'Hiking', 'Backpacking (hiking)'], ['Hiking'], ['News'], ['News'], ['News'], ['Moccasin', 'Gear', 'Sneakers', 'Shoe', 'Sandal'], ['Shoe', 'Nike Air Max', 'Sneakers'], ['Vibram', 'Nike ACG', 'Adidas'], ['Mother'], ['Mother'], ['Italy'], ['Italy'], ['Adidas']], [['Dress', 'Undershirt', 'Cufflink', 'Fashion accessory', 'Sneakers', 'Dress shirt', 'Bow tie', 'Barbecue', 'Shirt', 'Grey', 'Suit', 'Slip-on shoe', 'White', 'Clothing', 'Blazer', 'Dress socks', 'Shawl', 'Sock', 'Sand', 'Sport coat', 'Belt (clothing)', 'Necktie', 'Button', 'Trousers', 'Shorts', 'Handkerchief', 'Cummerbund', 'Collar (clothing)', 'T-shirt', 'Shoe', 'Jacket', 'Waistcoat', 'Hat', 'Sandal', 'Cocktail dress', 'Black tie', 'Beige'], ['Folk music'], ['Folk music'], ['Bruno Mars'], ['Bruno Mars'], ['Khaki', 'Chino cloth', 'Denim'], ['Dress', 'Sock', 'Fashion accessory', 'Textile', 'Clothing', 'Patent', 'T-shirt', 'Belt (clothing)', 'Shoe', 'Hat', 'Water supply', 'Glass'], ['France', 'United States'], ['Shoe', 'Fashion accessory', 'Sneakers'], ['Cowboy'], ['Black tie'], ['Clothing'], ['Linen'], ['Esquire']], [['Footwear', 'Shoe', 'Paper', 'Fashion', 'Clothing'], ['Footwear', 'Sneakers', 'Shoe', 'Toilet paper', 'Trousers', 'Clothing'], ['Shoe', 'Toilet paper', 'Sneakers'], ['West Coast of the United States'], ['Old-school hip hop'], ['Fashion', 'Clothing'], ['Fashion'], ['Fashion'], ['Fashion'], ['Esquire', 'Fashion']], [['Shoe', 'Slip-on shoe', 'Sneakers'], ['Shoe', 'Sneakers'], ['Shoe'], ['New York City'], ['God'], ['Germany'], ['Tony the Tiger'], ['Esquire', 'Army Trainer', 'WFH']], [['Barbecue', 'Shirt', 'Flip-flops', 'Glasses', 'Sneakers', 'Sport coat', 'Shoe', 'Slip-on shoe', 'Shorts', 'Clothing'], ['God'], ['Linen', 'Cotton', 'Nylon'], ['Shoe', 'Sneakers'], ['Hiking', 'Shoe', 'Clothing'], ['Clothing'], ['Hiking'], ['Persol', 'Esquire'], ['Italy'], ['Italy']]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [09/Aug/2020 12:58:12] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:58:23] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:58:34] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:58:48] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Aug/2020 12:59:18] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}